{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467d4228",
   "metadata": {},
   "source": [
 
    "\n",
    "Python 3.8, Ubuntu and CUDA 12 are required. A clean Anaconda/Conda environment is essential. Refer to setup instructions and QDNABERT2_working_env.yaml file.\n",
    "\n",
    "\n",
    "Linear NN as final BERT layer to make comparison fairer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a77bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c \"nvidia/label/cuda-12.2.2\" cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd1fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\r\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\r\n",
      "Cuda compilation tools, release 12.2, V12.2.140\r\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64114bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install triton==2.0.0.dev20221202\n",
    "! pip install torch==1.13.1\n",
    "! pip install einops==0.6.1\n",
    "! pip install peft==0.4.0\n",
    "! pip install huggingface-hub==0.16.4\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26549e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorboard==2.13.0\n",
    "! pip install tensorboard-data-server==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a33270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/beri/anaconda3/envs/QDNABERT2env\n"
     ]
    }
   ],
   "source": [
    "### SET DIRECTORY\n",
    "import os\n",
    "os.chdir(\"/home/beri/anaconda3/envs/QDNABERT2env\") # my directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d44a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/zhihan1996/DNABERT-2-117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedbe4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "### LOAD PYTHON MODULES\n",
    "# Load basic modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "# Load data and machine learning modules\n",
    "import torch\n",
    "import triton\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Print triton version\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34e5f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "### PRINT GPU DEVICE\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c602f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â LOAD DNABERT MODULE\n",
    "# In the github I already uploaded this: https://github.com/Zhihan1996/DNABERT_2\n",
    "# Note: I modified the file DNABERT_2/finetune/train.py to solve some bugs.\n",
    "\n",
    "sys.path.append(\"/home/beri/anaconda3/envs/QDNABERT2env/finetune/\") \n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5067fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "model_args=ModelArguments()\n",
    "data_args=DataArguments()\n",
    "training_args=TrainingArguments\n",
    "\n",
    "# better to save the pretrained model \"DNABERT-2-117M\" somewhere locally\n",
    "model_args.model_name_or_path=\"/home/beri/anaconda3/envs/QDNABERT2env/DNABERT-2-117M/\"\n",
    "\n",
    "batchsize=16 # reduce it to decrease CUDA memory\n",
    "\n",
    "training_args.deepspeed_plugin=None\n",
    "#training_args.log_level=\"info\"\n",
    "training_args.run_name=\"DNABERT2_aug\"\n",
    "training_args.model_max_length=20\n",
    "training_args.per_device_train_batch_size=batchsize\n",
    "training_args.per_device_eval_batch_size=batchsize\n",
    "training_args.gradient_accumulation_steps=5 # increase it to reduce CUDA memory \n",
    "training_args.learning_rate=3e-5\n",
    "training_args.num_train_epochs=4\n",
    "training_args.fp16=False\n",
    "training_args.save_steps=0 #400\n",
    "training_args.evaluation_strategy=\"steps\"\n",
    "training_args.eval_steps=500 # avoid testing on validation while training too frequently (takes a lot of memory)\n",
    "training_args.warmup_steps=50\n",
    "training_args.logging_steps=100000\n",
    "training_args.find_unused_parameters=False\n",
    "\n",
    "# Other arguments to add since it was bugging\n",
    "training_args.device=torch.device('cuda:0')\n",
    "training_args.report_to=[\"tensorboard\"]\n",
    "training_args.world_size=1\n",
    "training_args.per_device_train_batch_size=8\n",
    "training_args.train_batch_size=batchsize\n",
    "training_args.eval_batch_size=batchsize\n",
    "training_args.test_batch_size=batchsize\n",
    "training_args.batch_size=batchsize\n",
    "training_args.num_training_steps=200\n",
    "training_args.n_gpu=1\n",
    "training_args.distributed_state=None\n",
    "training_args.local_rank=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9894c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec32290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the linear classifier layer\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class customLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(customLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(768,2)\n",
    "        self.linear1.weight = torch.nn.Parameter(torch.zeros(768,2))\n",
    "        self.linear1.bias = torch.nn.Parameter(torch.ones(2))\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        h = self.linear1(input_array)\n",
    "        return self.linear1(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0a68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/beri/anaconda3/envs/QDNABERT2env/DNABERT-2-117M/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): customLayer(\n",
      "    (linear1): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "117070082 parameters\n"
     ]
    }
   ],
   "source": [
    "# MAKE CUSTOM MODEL TO MODIFY DNABERT2\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "  def __init__(self,num_labels): \n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    self.model = AutoModel.from_pretrained(model_args.model_name_or_path, \n",
    "                                           trust_remote_code=True, output_hidden_states=True).cuda()\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    \n",
    "    # QUANTUM LAYER \n",
    "    self.classifier = customLayer() \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the body\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #Add custom layers\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "    print(sequence_output.shape)\n",
    "    \n",
    "    # By default, no pooling is done, only the first word is taken (sequence_output[:,0,:]). \n",
    "    # The authors of BERT paper found it sufficient to use only the output from the 1st token \n",
    "    # for few tasks such as classification\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "    \n",
    "    # POOLING FOR QUANTUM TEAM\n",
    "    #sequence_output_max=torch.max(sequence_output, dim=1) # here global max pooling\n",
    "    #logits = self.classifier(sequence_output_max)\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs[0])\n",
    "\n",
    "modeltest=CustomModel(2).cuda()\n",
    "print(modeltest)\n",
    "\n",
    "total_params = sum(p.numel() for p in modeltest.parameters())\n",
    "print(str(total_params)+\" parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3272313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/beri/anaconda3/envs/QDNABERT2env/DNABERT-2-117M/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight\n",
      "model.embeddings.token_type_embeddings.weight\n",
      "model.embeddings.LayerNorm.weight\n",
      "model.embeddings.LayerNorm.bias\n",
      "model.encoder.layer.0.attention.self.Wqkv.weight\n",
      "model.encoder.layer.0.attention.self.Wqkv.bias\n",
      "model.encoder.layer.0.attention.output.dense.weight\n",
      "model.encoder.layer.0.attention.output.dense.bias\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.0.mlp.gated_layers.weight\n",
      "model.encoder.layer.0.mlp.wo.weight\n",
      "model.encoder.layer.0.mlp.wo.bias\n",
      "model.encoder.layer.0.mlp.layernorm.weight\n",
      "model.encoder.layer.0.mlp.layernorm.bias\n",
      "model.encoder.layer.1.attention.self.Wqkv.weight\n",
      "model.encoder.layer.1.attention.self.Wqkv.bias\n",
      "model.encoder.layer.1.attention.output.dense.weight\n",
      "model.encoder.layer.1.attention.output.dense.bias\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.1.mlp.gated_layers.weight\n",
      "model.encoder.layer.1.mlp.wo.weight\n",
      "model.encoder.layer.1.mlp.wo.bias\n",
      "model.encoder.layer.1.mlp.layernorm.weight\n",
      "model.encoder.layer.1.mlp.layernorm.bias\n",
      "model.encoder.layer.2.attention.self.Wqkv.weight\n",
      "model.encoder.layer.2.attention.self.Wqkv.bias\n",
      "model.encoder.layer.2.attention.output.dense.weight\n",
      "model.encoder.layer.2.attention.output.dense.bias\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.2.mlp.gated_layers.weight\n",
      "model.encoder.layer.2.mlp.wo.weight\n",
      "model.encoder.layer.2.mlp.wo.bias\n",
      "model.encoder.layer.2.mlp.layernorm.weight\n",
      "model.encoder.layer.2.mlp.layernorm.bias\n",
      "model.encoder.layer.3.attention.self.Wqkv.weight\n",
      "model.encoder.layer.3.attention.self.Wqkv.bias\n",
      "model.encoder.layer.3.attention.output.dense.weight\n",
      "model.encoder.layer.3.attention.output.dense.bias\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.3.mlp.gated_layers.weight\n",
      "model.encoder.layer.3.mlp.wo.weight\n",
      "model.encoder.layer.3.mlp.wo.bias\n",
      "model.encoder.layer.3.mlp.layernorm.weight\n",
      "model.encoder.layer.3.mlp.layernorm.bias\n",
      "model.encoder.layer.4.attention.self.Wqkv.weight\n",
      "model.encoder.layer.4.attention.self.Wqkv.bias\n",
      "model.encoder.layer.4.attention.output.dense.weight\n",
      "model.encoder.layer.4.attention.output.dense.bias\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.4.mlp.gated_layers.weight\n",
      "model.encoder.layer.4.mlp.wo.weight\n",
      "model.encoder.layer.4.mlp.wo.bias\n",
      "model.encoder.layer.4.mlp.layernorm.weight\n",
      "model.encoder.layer.4.mlp.layernorm.bias\n",
      "model.encoder.layer.5.attention.self.Wqkv.weight\n",
      "model.encoder.layer.5.attention.self.Wqkv.bias\n",
      "model.encoder.layer.5.attention.output.dense.weight\n",
      "model.encoder.layer.5.attention.output.dense.bias\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.5.mlp.gated_layers.weight\n",
      "model.encoder.layer.5.mlp.wo.weight\n",
      "model.encoder.layer.5.mlp.wo.bias\n",
      "model.encoder.layer.5.mlp.layernorm.weight\n",
      "model.encoder.layer.5.mlp.layernorm.bias\n",
      "model.encoder.layer.6.attention.self.Wqkv.weight\n",
      "model.encoder.layer.6.attention.self.Wqkv.bias\n",
      "model.encoder.layer.6.attention.output.dense.weight\n",
      "model.encoder.layer.6.attention.output.dense.bias\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.6.mlp.gated_layers.weight\n",
      "model.encoder.layer.6.mlp.wo.weight\n",
      "model.encoder.layer.6.mlp.wo.bias\n",
      "model.encoder.layer.6.mlp.layernorm.weight\n",
      "model.encoder.layer.6.mlp.layernorm.bias\n",
      "model.encoder.layer.7.attention.self.Wqkv.weight\n",
      "model.encoder.layer.7.attention.self.Wqkv.bias\n",
      "model.encoder.layer.7.attention.output.dense.weight\n",
      "model.encoder.layer.7.attention.output.dense.bias\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.7.mlp.gated_layers.weight\n",
      "model.encoder.layer.7.mlp.wo.weight\n",
      "model.encoder.layer.7.mlp.wo.bias\n",
      "model.encoder.layer.7.mlp.layernorm.weight\n",
      "model.encoder.layer.7.mlp.layernorm.bias\n",
      "model.encoder.layer.8.attention.self.Wqkv.weight\n",
      "model.encoder.layer.8.attention.self.Wqkv.bias\n",
      "model.encoder.layer.8.attention.output.dense.weight\n",
      "model.encoder.layer.8.attention.output.dense.bias\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.8.mlp.gated_layers.weight\n",
      "model.encoder.layer.8.mlp.wo.weight\n",
      "model.encoder.layer.8.mlp.wo.bias\n",
      "model.encoder.layer.8.mlp.layernorm.weight\n",
      "model.encoder.layer.8.mlp.layernorm.bias\n",
      "model.encoder.layer.9.attention.self.Wqkv.weight\n",
      "model.encoder.layer.9.attention.self.Wqkv.bias\n",
      "model.encoder.layer.9.attention.output.dense.weight\n",
      "model.encoder.layer.9.attention.output.dense.bias\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.9.mlp.gated_layers.weight\n",
      "model.encoder.layer.9.mlp.wo.weight\n",
      "model.encoder.layer.9.mlp.wo.bias\n",
      "model.encoder.layer.9.mlp.layernorm.weight\n",
      "model.encoder.layer.9.mlp.layernorm.bias\n",
      "model.encoder.layer.10.attention.self.Wqkv.weight\n",
      "model.encoder.layer.10.attention.self.Wqkv.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.mlp.gated_layers.weight\n",
      "model.encoder.layer.10.mlp.wo.weight\n",
      "model.encoder.layer.10.mlp.wo.bias\n",
      "model.encoder.layer.10.mlp.layernorm.weight\n",
      "model.encoder.layer.10.mlp.layernorm.bias\n",
      "model.encoder.layer.11.attention.self.Wqkv.weight\n",
      "model.encoder.layer.11.attention.self.Wqkv.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.mlp.gated_layers.weight\n",
      "model.encoder.layer.11.mlp.wo.weight\n",
      "model.encoder.layer.11.mlp.wo.bias\n",
      "model.encoder.layer.11.mlp.layernorm.weight\n",
      "model.encoder.layer.11.mlp.layernorm.bias\n",
      "model.pooler.dense.weight\n",
      "model.pooler.dense.bias\n",
      "classifier.linear1.weight\n",
      "classifier.linear1.bias\n"
     ]
    }
   ],
   "source": [
    "# please check the below is correct - BERT parameters will be frozen and only quantum layer will be trained\n",
    "# although the model is working there could still be errors here in terms of incorrect class inheritance, \n",
    "# so please check this code here very carefully\n",
    "# you are also free to rewrite the code into a cleaner and perhaps more effective way\n",
    "\n",
    "model = CustomModel(nn.Module)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f03d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PICK GUE DATA CLASS\n",
    "GUE_class=\"tf\" # pick among: \"EMP\" \"mouse\" \"prom\" \"splice\" \"tf\" \"virus\"\n",
    "\n",
    "if GUE_class==\"EMP\":\n",
    "    GUE_subclasses = [\"H3\",\"H3K14ac\",\"H3K36me3\",\"H3K4me1\",\"H3K4me2\",\"H3K4me3\",\"H3K79me3\",\"H3K9ac\",\"H4\",\"H4ac\"]\n",
    "\n",
    "if GUE_class==\"mouse\":\n",
    "    GUE_subclasses = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "if GUE_class==\"prom\":\n",
    "    GUE_subclasses = [\"prom_300_all\",\"prom_300_notata\",\"prom_300_tata\",\n",
    "                      \"prom_core_all\",\"prom_core_notata\",\"prom_core_tata\"]\n",
    "\n",
    "if GUE_class==\"splice\":\n",
    "    GUE_subclasses = [\"reconstructed\"]\n",
    "\n",
    "if GUE_class==\"tf\":\n",
    "    GUE_subclasses = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "if GUE_class==\"virus\":\n",
    "    GUE_subclasses = [\"covid\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77f2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702a6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Perform single sequence classification...\n",
      "WARNING:root:Perform single sequence classification...\n",
      "WARNING:root:Perform single sequence classification...\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/beri/anaconda3/envs/QDNABERT2env/DNABERT-2-117M/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Currently training with a batch size of: 16\n",
      "***** Running training *****\n",
      "  Num examples = 32,378\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Training with DataParallel so batch size has been adjusted to: 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 5\n",
      "  Total optimization steps = 1,616\n",
      "  Number of trainable parameters = 117,070,082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x768 and 2x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 74\u001b[0m\n\u001b[1;32m     66\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     67\u001b[0m                                tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     68\u001b[0m                                args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     72\u001b[0m                                data_collator\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[1;32m     73\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlocal_rank\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlocal_rank\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39msave_model:\n\u001b[1;32m     77\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_state()\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1838\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1844\u001b[0m ):\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/transformers/trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2693\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2696\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/transformers/trainer.py:2718\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2717\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2718\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(sequence_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# By default, no pooling is done, only the first word is taken (sequence_output[:,0,:]). \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# The authors of BERT paper found it sufficient to use only the output from the 1st token \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# for few tasks such as classification\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calculate losses\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# POOLING FOR QUANTUM TEAM\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#sequence_output_max=torch.max(sequence_output, dim=1) # here global max pooling\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#logits = self.classifier(sequence_output_max)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mcustomLayer.forward\u001b[0;34m(self, input_array)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_array):\n\u001b[0;32m---> 15\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(h)\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/QDNABERT2env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x768 and 2x768)"
     ]
    }
   ],
   "source": [
    "### FINE TUNE DNABERT2\n",
    "\n",
    "\n",
    "for GUE_subclass in GUE_subclasses:\n",
    "    \n",
    "    #GUE_subclass=\"H3K4me1\"\n",
    "\n",
    "    data_args.data_path=\"/home/beri/anaconda3/envs/QDNABERT2env/GUE/\"+GUE_class+\"/\"+GUE_subclass\n",
    "    training_args.output_dir=\"results/DNABERT2/\"+GUE_class+\"/\"+GUE_subclass\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    # load model\n",
    "    # model used by defaults (no model customization)\n",
    "    if False:\n",
    "        model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            num_labels=train_dataset.num_labels,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    # customised model\n",
    "    if True:\n",
    "        model=CustomModel(num_labels=2).cuda()\n",
    "\n",
    "    # configure LoRA\n",
    "    if model_args.use_lora:\n",
    "        lora_config = LoraConfig(\n",
    "            r=model_args.lora_r,\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # define trainer\n",
    "    trainer = transformers.Trainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=val_dataset,\n",
    "                                   data_collator=data_collator)\n",
    "    trainer.local_rank=training_args.local_rank\n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "        \n",
    "    \n",
    "    ### TEST ACCURACY ON INDEPENDENT TEST DATA\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"metrics\")\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"test_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "    ### ONLY IF YOU WANT TO SAVE THE FINE-TUNED MODEL (BY DEFAULT: NOT SAVING)\n",
    "    if False:\n",
    "        path_model=\"/home/beri/anaconda3/envs/QDNABERT2env/pytorch_model_finetuned.bin\"\n",
    "        torch.save(model.state_dict(), path_model)\n",
    "    \n",
    "    del tokenizer, train_dataset, val_dataset, test_dataset, data_collator, model, trainer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871205ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed503be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
