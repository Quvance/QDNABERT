{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a77bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c \"nvidia/label/cuda-12.2.2\" cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64114bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install triton==2.0.0.dev20221202\n",
    "! pip install torch==1.13.1\n",
    "! pip install einops==0.6.1\n",
    "! pip install peft==0.4.0\n",
    "! pip install huggingface-hub==0.16.4\n",
    "! pip install numpy==1.24.4\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26549e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorboard==2.13.0\n",
    "! pip install tensorboard-data-server==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a33270",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET DIRECTORY\n",
    "import os\n",
    "os.chdir(\"/home/beri/anaconda3/envs/QDNABERT2env\") # my directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d44a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/zhihan1996/DNABERT-2-117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbe4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD PYTHON MODULES\n",
    "# Load basic modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "# Load data and machine learning modules\n",
    "import torch\n",
    "import triton\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Print triton version\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRINT GPU DEVICE\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c602f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â LOAD DNABERT MODULE\n",
    "# In the github I already uploaded this: https://github.com/Zhihan1996/DNABERT_2\n",
    "# Note: I modified the file DNABERT_2/finetune/train.py to solve some bugs.\n",
    "\n",
    "sys.path.append(\"/home/beri/anaconda3/envs/QDNABERT2env/finetune/\") \n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "model_args=ModelArguments()\n",
    "data_args=DataArguments()\n",
    "training_args=TrainingArguments\n",
    "\n",
    "# better to save the pretrained model \"DNABERT-2-117M\" somewhere locally\n",
    "model_args.model_name_or_path=\"/home/beri/anaconda3/envs/QDNABERT2env/DNABERT-2-117M/\"\n",
    "\n",
    "batchsize=16 # reduce it to decrease CUDA memory\n",
    "\n",
    "training_args.deepspeed_plugin=None\n",
    "#training_args.log_level=\"info\"\n",
    "training_args.run_name=\"DNABERT2_aug\"\n",
    "training_args.model_max_length=20\n",
    "training_args.per_device_train_batch_size=batchsize\n",
    "training_args.per_device_eval_batch_size=batchsize\n",
    "training_args.gradient_accumulation_steps=5 # increase it to reduce CUDA memory \n",
    "training_args.learning_rate=3e-5\n",
    "training_args.num_train_epochs=4\n",
    "training_args.fp16=False\n",
    "training_args.save_steps=0 #400\n",
    "training_args.evaluation_strategy=\"steps\"\n",
    "training_args.eval_steps=500 # avoid testing on validation while training too frequently (takes a lot of memory)\n",
    "training_args.warmup_steps=50\n",
    "training_args.logging_steps=100000\n",
    "training_args.find_unused_parameters=False\n",
    "\n",
    "# Other arguments to add since it was bugging\n",
    "training_args.device=torch.device('cuda:0')\n",
    "training_args.report_to=[\"tensorboard\"]\n",
    "training_args.world_size=1\n",
    "training_args.per_device_train_batch_size=8\n",
    "training_args.train_batch_size=batchsize\n",
    "training_args.eval_batch_size=batchsize\n",
    "training_args.test_batch_size=batchsize\n",
    "training_args.batch_size=batchsize\n",
    "training_args.num_training_steps=200\n",
    "training_args.n_gpu=1\n",
    "training_args.distributed_state=None\n",
    "training_args.local_rank=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE CUSTOM MODEL TO MODIFY DNABERT2\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "  def __init__(self,num_labels): \n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "    self.model = AutoModel.from_pretrained(model_args.model_name_or_path, \n",
    "                                           trust_remote_code=True, output_hidden_states=True).cuda()\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    \n",
    "    # TO QUANTUM TEAM: dummy classification layer to replace with quantum layer\n",
    "    self.classifier = nn.Linear(768,num_labels) \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the body\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #Add custom layers\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "    print(sequence_output.shape)\n",
    "    \n",
    "    # By default, no pooling is done, only the first word is taken (sequence_output[:,0,:]). \n",
    "    # The authors of BERT paper found it sufficient to use only the output from the 1st token \n",
    "    # for few tasks such as classification\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "    \n",
    "    # POOLING FOR QUANTUM TEAM\n",
    "    #sequence_output_max=torch.max(sequence_output, dim=1) # here global max pooling\n",
    "    #logits = self.classifier(sequence_output_max)\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs[0])\n",
    "\n",
    "modeltest=CustomModel(2).cuda()\n",
    "print(modeltest)\n",
    "\n",
    "total_params = sum(p.numel() for p in modeltest.parameters())\n",
    "print(str(total_params)+\" parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f03d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PICK GUE DATA CLASS\n",
    "GUE_class=\"tf\" # pick among: \"EMP\" \"mouse\" \"prom\" \"splice\" \"tf\" \"virus\"\n",
    "\n",
    "if GUE_class==\"EMP\":\n",
    "    GUE_subclasses = [\"H3\",\"H3K14ac\",\"H3K36me3\",\"H3K4me1\",\"H3K4me2\",\"H3K4me3\",\"H3K79me3\",\"H3K9ac\",\"H4\",\"H4ac\"]\n",
    "\n",
    "if GUE_class==\"mouse\":\n",
    "    GUE_subclasses = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "if GUE_class==\"prom\":\n",
    "    GUE_subclasses = [\"prom_300_all\",\"prom_300_notata\",\"prom_300_tata\",\n",
    "                      \"prom_core_all\",\"prom_core_notata\",\"prom_core_tata\"]\n",
    "\n",
    "if GUE_class==\"splice\":\n",
    "    GUE_subclasses = [\"reconstructed\"]\n",
    "\n",
    "if GUE_class==\"tf\":\n",
    "    GUE_subclasses = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "if GUE_class==\"virus\":\n",
    "    GUE_subclasses = [\"covid\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINE TUNE DNABERT2\n",
    "\n",
    "\n",
    "for GUE_subclass in GUE_subclasses:\n",
    "    \n",
    "    #GUE_subclass=\"H3K4me1\"\n",
    "\n",
    "    data_args.data_path=\"/home/beri/anaconda3/envs/QDNABERT2env/GUE/\"+GUE_class+\"/\"+GUE_subclass\n",
    "    training_args.output_dir=\"results/DNABERT2/\"+GUE_class+\"/\"+GUE_subclass\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if \"InstaDeepAI\" in model_args.model_name_or_path:\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "\n",
    "    # define datasets and data collator\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                      data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                      kmer=data_args.kmer)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                     data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                     kmer=data_args.kmer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    # load model\n",
    "    # model used by defaults (no model customization)\n",
    "    if True:\n",
    "        model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            num_labels=train_dataset.num_labels,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "    # customised model\n",
    "    if False:\n",
    "        model=CustomModel(num_labels=2).cuda()\n",
    "\n",
    "    # configure LoRA\n",
    "    if model_args.use_lora:\n",
    "        lora_config = LoraConfig(\n",
    "            r=model_args.lora_r,\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # define trainer\n",
    "    trainer = transformers.Trainer(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   args=training_args,\n",
    "                                   compute_metrics=compute_metrics,\n",
    "                                   train_dataset=train_dataset,\n",
    "                                   eval_dataset=val_dataset,\n",
    "                                   data_collator=data_collator)\n",
    "    trainer.local_rank=training_args.local_rank\n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.save_model:\n",
    "        trainer.save_state()\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "        \n",
    "    \n",
    "    ### TEST ACCURACY ON INDEPENDENT TEST DATA\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "    if training_args.eval_and_save_results:\n",
    "        results_path = os.path.join(training_args.output_dir, \"metrics\")\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"test_results.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "    ### ONLY IF YOU WANT TO SAVE THE FINE-TUNED MODEL (BY DEFAULT: NOT SAVING)\n",
    "    if False:\n",
    "        path_model=\"/home/beri/anaconda3/envs/QDNABERT2env/pytorch_model_finetuned.bin\"\n",
    "        torch.save(model.state_dict(), path_model)\n",
    "    \n",
    "    del tokenizer, train_dataset, val_dataset, test_dataset, data_collator, model, trainer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871205ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
